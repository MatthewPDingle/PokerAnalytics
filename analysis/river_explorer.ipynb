{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6013eec",
   "metadata": {},
   "source": [
    "# River Barrel Explorer\n",
    "\n",
    "Explore river betting lines (triple barrels, flop-river checks, delayed barrels) to spot which holdings fuel bluffs versus value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ca036",
   "metadata": {},
   "source": [
    "## Line Definitions\n",
    "\n",
    "- **Triple Barrel** – player bet (or raised) on flop, turn, and barrelled the river.\n",
    "- **Turn Follow-up** – player skipped the flop c-bet but bet (or raised) turn and followed with a river bet.\n",
    "- **Flop-River** – player c-bet the flop, skipped betting the turn, then fired the river.\n",
    "- **River Only** – player did not bet flop or turn, only led on the river.\n",
    "- **Missed Draws** – any busted draw is counted under Air (tracked separately for drilldowns but not shown as its own row).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6e15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def _locate_project_root() -> Path:\n",
    "    current = Path().resolve()\n",
    "    for candidate in (current, *current.parents):\n",
    "        if (candidate / \"AGENTS.md\").exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Repository root not found from notebook location.\")\n",
    "\n",
    "PROJECT_ROOT = _locate_project_root()\n",
    "del _locate_project_root\n",
    "\n",
    "DB_CANDIDATES = [\n",
    "    Path(r\"T:\\Dev\\ignition\\drivehud\\drivehud.db\"),\n",
    "    Path(\"/mnt/t/Dev/ignition/drivehud/drivehud.db\"),\n",
    "    PROJECT_ROOT / \"drivehud\" / \"drivehud.db\",\n",
    "]\n",
    "\n",
    "for candidate in DB_CANDIDATES:\n",
    "    if candidate.exists():\n",
    "        DB_PATH = candidate\n",
    "        break\n",
    "else:\n",
    "    checked = os.linesep.join(str(p) for p in DB_CANDIDATES)\n",
    "    message = \"Database not found. Checked:\" + os.linesep + checked\n",
    "    raise FileNotFoundError(message)\n",
    "\n",
    "CACHE_PATH = PROJECT_ROOT / \"analysis\" / \"cache\" / \"river_events.json\"\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Database not found at {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c929093d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.path:\n\u001b[32m      3\u001b[39m     sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(PROJECT_ROOT))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01manalysis\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mriver_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     RIVER_BUCKETS,\n\u001b[32m     10\u001b[39m     available_primary_categories \u001b[38;5;28;01mas\u001b[39;00m available_river_categories,\n\u001b[32m     11\u001b[39m     load_river_events,\n\u001b[32m     12\u001b[39m     river_response_events,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from analysis.river_utils import (\n",
    "    RIVER_BUCKETS,\n",
    "    available_primary_categories as available_river_categories,\n",
    "    load_river_events,\n",
    "    river_response_events,\n",
    ")\n",
    "from analysis.cbet_utils import BASE_PRIMARY_CATEGORIES\n",
    "from matplotlib.colors import LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48986379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "RIVER_BUCKET_BOUNDS = [\n",
    "    (0.00, 0.25),\n",
    "    (0.25, 0.40),\n",
    "    (0.40, 0.60),\n",
    "    (0.60, 0.80),\n",
    "    (0.80, 1.00),\n",
    "    (1.00, 1.25),\n",
    "    (1.25, float('inf')),\n",
    "]\n",
    "\n",
    "def _bucket_label(low: float, high: float) -> str:\n",
    "    if high == float('inf'):\n",
    "        return f\">={low:.2f}\"\n",
    "    return f\"[{low:.2f}, {high:.2f})\"\n",
    "\n",
    "RIVER_BUCKETS = [(low, high, _bucket_label(low, high)) for (low, high) in RIVER_BUCKET_BOUNDS]\n",
    "del _bucket_label\n",
    "\n",
    "PRIMARY_GROUPS = {cat: [cat] for cat in BASE_PRIMARY_CATEGORIES}\n",
    "PRIMARY_GROUPS['Missed Draw'] = ['Missed Draw']\n",
    "GROUPED_PRIMARY_ORDER = [\n",
    "    ('Events', None),\n",
    "    ('Air', ['Air']),\n",
    "    ('Missed Draw', ['Missed Draw']),\n",
    "    ('Weak Pair', ['Underpair', 'Bottom Pair', 'Middle Pair']),\n",
    "    ('Top Pair', ['Top Pair']),\n",
    "    ('Overpair', ['Overpair']),\n",
    "    ('Two Pair', ['Two Pair']),\n",
    "    ('Trips/Set', ['Trips/Set']),\n",
    "    ('Monster', ['Straight', 'Flush', 'Full House', 'Quads']),\n",
    "]\n",
    "\n",
    "FORCE_RELOAD = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee043bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = load_river_events(DB_PATH, cache_path=CACHE_PATH, force=FORCE_RELOAD)\n",
    "print(f\"Loaded {len(events)} river bet events.\")\n",
    "print(\"Line types:\", \", \".join(sorted({e[\"line_type\"] for e in events})))\n",
    "available_categories = available_river_categories(events)\n",
    "print(\"Observed primary categories:\", \", \".join(available_categories))\n",
    "\n",
    "events_df = pd.DataFrame(events)\n",
    "responses_df = pd.DataFrame(river_response_events(events))\n",
    "\n",
    "bucket_labels = [b[2] for b in RIVER_BUCKETS]\n",
    "bucket_edges = [b[0] for b in RIVER_BUCKETS] + [RIVER_BUCKETS[-1][1]]\n",
    "events_df[\"bucket\"] = pd.cut(events_df[\"ratio\"], bins=bucket_edges, labels=bucket_labels, right=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66539d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAND_ORDER = ['Air'] + [cat for cat in BASE_PRIMARY_CATEGORIES if cat != 'Air']\n",
    "LINE_DISPLAY_ORDER = ['Triple Barrel', 'Flop-River', 'Turn Follow-up', 'River Only', 'Other']\n",
    "\n",
    "SPECIAL_FIELD_CANDIDATES = {\n",
    "    'All-In': ('is_all_in', 'is_all_in_bet', 'is_all_in_river'),\n",
    "    '1 BB': ('is_one_bb', 'is_one_bb_bet', 'is_one_bb_river'),\n",
    "}\n",
    "\n",
    "\n",
    "def _select_flag_field(df, candidates):\n",
    "    for field in candidates:\n",
    "        if field in df.columns:\n",
    "            return field\n",
    "    return None\n",
    "\n",
    "\n",
    "def _subset_for_flag(df, candidates):\n",
    "    field = _select_flag_field(df, candidates)\n",
    "    if field is None:\n",
    "        return None\n",
    "    subset = df[df[field]]\n",
    "    return subset if not subset.empty else None\n",
    "\n",
    "\n",
    "def _style_bucket_table(summary_df, caption, special_cols):\n",
    "    styled = (\n",
    "        summary_df.style\n",
    "        .format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "        .format('{:.1f}%', subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .background_gradient(cmap='YlOrRd', axis=None, subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .set_caption(caption)\n",
    "    )\n",
    "    if special_cols:\n",
    "        first = special_cols[0]\n",
    "        idx = summary_df.columns.get_loc(first)\n",
    "        styled = styled.set_table_styles([\n",
    "            {'selector': f'th.col_heading.level0.col{idx}', 'props': [('border-left', '2px solid #64748b')]},\n",
    "            {'selector': f'td.col{idx}', 'props': [('border-left', '2px solid #64748b')]},\n",
    "        ], overwrite=False)\n",
    "    return styled\n",
    "\n",
    "\n",
    "\n",
    "def summarize_line(df):\n",
    "    total = len(df)\n",
    "    summary = {'Events': float(total)}\n",
    "    for cat in HAND_ORDER:\n",
    "        summary[cat] = df['primary'].eq(cat).mean() * 100 if total else 0.0\n",
    "    return summary\n",
    "\n",
    "def summarize_line_grouped(df):\n",
    "    total = len(df)\n",
    "    grouped = {'Events': float(total)}\n",
    "    for label, members in GROUPED_PRIMARY_ORDER:\n",
    "        if members is None or label == 'Events':\n",
    "            continue\n",
    "        grouped[label] = df['primary'].isin(members).mean() * 100 if total else 0.0\n",
    "    return grouped\n",
    "\n",
    "def display_line_comparison(df, caption):\n",
    "    data = {}\n",
    "    for line in LINE_DISPLAY_ORDER:\n",
    "        subset = df[df['line_type'] == line]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        data[line] = summarize_line(subset)\n",
    "    if not data:\n",
    "        print('No data for requested lines.')\n",
    "        return\n",
    "    summary_df = pd.DataFrame(data).reindex(['Events'] + HAND_ORDER)\n",
    "    styled = (\n",
    "        summary_df.style\n",
    "        .format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "        .format('{:.1f}%', subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .background_gradient(cmap='YlOrRd', axis=None, subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .set_caption(caption)\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def display_line_comparison_grouped(df, caption):\n",
    "    data = {}\n",
    "    for line in LINE_DISPLAY_ORDER:\n",
    "        subset = df[df['line_type'] == line]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        data[line] = summarize_line_grouped(subset)\n",
    "    if not data:\n",
    "        print('No data for requested lines.')\n",
    "        return\n",
    "    summary_df = pd.DataFrame(data).reindex(['Events'] + [label for label, members in GROUPED_PRIMARY_ORDER if members is not None])\n",
    "    styled = (\n",
    "        summary_df.style\n",
    "        .format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "        .format('{:.1f}%', subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .background_gradient(cmap='YlOrRd', axis=None, subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .set_caption(caption)\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "def summarize_bucket(df):\n",
    "    total = len(df)\n",
    "    result = {'Events': float(total)}\n",
    "    for cat in HAND_ORDER:\n",
    "        result[cat] = df['primary'].eq(cat).mean() * 100 if total else 0.0\n",
    "    return result\n",
    "\n",
    "def summarize_bucket_grouped(df):\n",
    "    total = len(df)\n",
    "    result = {'Events': float(total)}\n",
    "    for label, members in GROUPED_PRIMARY_ORDER:\n",
    "        if members is None or label == 'Events':\n",
    "            continue\n",
    "        result[label] = df['primary'].isin(members).mean() * 100 if total else 0.0\n",
    "    return result\n",
    "\n",
    "\n",
    "def display_bucket_table(df, caption):\n",
    "    bucket_labels_local = [bucket[2] for bucket in RIVER_BUCKETS]\n",
    "    rows = []\n",
    "    grouped_rows = []\n",
    "    index = []\n",
    "    for bucket_label in bucket_labels_local:\n",
    "        subset = df[df['bucket'] == bucket_label]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        rows.append(summarize_bucket(subset))\n",
    "        grouped_rows.append(summarize_bucket_grouped(subset))\n",
    "        index.append(bucket_label)\n",
    "    if not rows:\n",
    "        print('No data for selected line.')\n",
    "        return\n",
    "    summary_df = pd.DataFrame(rows, index=index).T\n",
    "    summary_df = summary_df.reindex(['Events'] + HAND_ORDER)\n",
    "    special_cols = []\n",
    "    for label, candidates in SPECIAL_FIELD_CANDIDATES.items():\n",
    "        subset = _subset_for_flag(df, candidates)\n",
    "        if subset is None:\n",
    "            continue\n",
    "        series = pd.Series(summarize_bucket(subset))\n",
    "        summary_df[label] = series.reindex(summary_df.index)\n",
    "        special_cols.append(label)\n",
    "    ordered_columns = [col for col in bucket_labels_local if col in summary_df.columns]\n",
    "    ordered_columns += [col for col in special_cols if col in summary_df.columns]\n",
    "    summary_df = summary_df[ordered_columns]\n",
    "    display(_style_bucket_table(summary_df, caption, special_cols))\n",
    "\n",
    "    grouped_df = pd.DataFrame(grouped_rows, index=index).T\n",
    "    grouped_df = grouped_df.reindex(['Events'] + [label for label, members in GROUPED_PRIMARY_ORDER if members is not None])\n",
    "    grouped_special = []\n",
    "    for label, candidates in SPECIAL_FIELD_CANDIDATES.items():\n",
    "        subset = _subset_for_flag(df, candidates)\n",
    "        if subset is None:\n",
    "            continue\n",
    "        series = pd.Series(summarize_bucket_grouped(subset))\n",
    "        grouped_df[label] = series.reindex(grouped_df.index)\n",
    "        grouped_special.append(label)\n",
    "    ordered_columns_grouped = [col for col in bucket_labels_local if col in grouped_df.columns]\n",
    "    ordered_columns_grouped += [col for col in grouped_special if col in grouped_df.columns]\n",
    "    grouped_df = grouped_df[ordered_columns_grouped]\n",
    "    display(_style_bucket_table(grouped_df, f\"{caption} (Grouped)\", grouped_special))\n",
    "\n",
    "def summarize_responses(df, response_type):\n",
    "    df = df[df['response'] == response_type].dropna(subset=['responder_primary'])\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "    data = {}\n",
    "    grouped = {}\n",
    "    for line in ['Triple Barrel', 'Flop-River', 'Turn Follow-up', 'River Only']:\n",
    "        subset = df[df['line_type'] == line]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        total = len(subset)\n",
    "        entry = {'Events': float(total)}\n",
    "        entry_grouped = {'Events': float(total)}\n",
    "        for cat in HAND_ORDER:\n",
    "            entry[cat] = subset['responder_primary'].eq(cat).mean() * 100 if total else 0.0\n",
    "        for label, members in GROUPED_PRIMARY_ORDER:\n",
    "            if members is None or label == 'Events':\n",
    "                continue\n",
    "            entry_grouped[label] = subset['responder_primary'].isin(members).mean() * 100 if total else 0.0\n",
    "        data[line] = entry\n",
    "        grouped[line] = entry_grouped\n",
    "    if not data:\n",
    "        return None, None\n",
    "    summary_df = pd.DataFrame(data).reindex(['Events'] + HAND_ORDER)\n",
    "    grouped_df = pd.DataFrame(grouped).reindex(['Events'] + [label for label, members in GROUPED_PRIMARY_ORDER if members is not None])\n",
    "    return summary_df, grouped_df\n",
    "\n",
    "def display_response_table(response_type):\n",
    "    summary_df, grouped_df = summarize_responses(responses_df, response_type)\n",
    "    if summary_df is None:\n",
    "        print(f'No responder data for {response_type} events.')\n",
    "        return\n",
    "    styled = (\n",
    "        summary_df.style\n",
    "        .format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "        .format('{:.1f}%', subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .background_gradient(cmap='PuBu', axis=None, subset=pd.IndexSlice[summary_df.index.difference(['Events']), :])\n",
    "        .set_caption(f'{response_type} vs River Bets (Responder Holdings)')\n",
    "    )\n",
    "    display(styled)\n",
    "\n",
    "    grouped_styled = (\n",
    "        grouped_df.style\n",
    "        .format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "        .format('{:.1f}%', subset=pd.IndexSlice[grouped_df.index.difference(['Events']), :])\n",
    "        .background_gradient(cmap='PuBu', axis=None, subset=pd.IndexSlice[grouped_df.index.difference(['Events']), :])\n",
    "        .set_caption(f'{response_type} vs River Bets (Responder Holdings) (Grouped)')\n",
    "    )\n",
    "    display(grouped_styled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ad3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _filter_standard_river_events(events_subset):\n",
    "    if events_subset is None or events_subset.empty:\n",
    "        return events_subset.iloc[0:0].copy()\n",
    "    mask = pd.Series(True, index=events_subset.index)\n",
    "    if 'is_all_in' in events_subset.columns:\n",
    "        mask &= ~events_subset['is_all_in']\n",
    "    if 'is_one_bb' in events_subset.columns:\n",
    "        mask &= ~events_subset['is_one_bb']\n",
    "    return events_subset.loc[mask].copy()\n",
    "\n",
    "\n",
    "def _filter_standard_river_responses(responses_subset):\n",
    "    if responses_subset is None or responses_subset.empty:\n",
    "        return responses_subset.iloc[0:0].copy()\n",
    "    return responses_subset.copy()\n",
    "\n",
    "\n",
    "def _safe_mean(series):\n",
    "    if series is None or len(series) == 0:\n",
    "        return 0.0\n",
    "    cleaned = series.dropna()\n",
    "    if cleaned.empty:\n",
    "        return 0.0\n",
    "    value = cleaned.mean()\n",
    "    return float(value) if pd.notna(value) else 0.0\n",
    "\n",
    "\n",
    "def _prepare_river_event_metrics(events_subset, responses_subset, *, exclude_special=True):\n",
    "    if events_subset is None or events_subset.empty:\n",
    "        return pd.DataFrame()\n",
    "    events_work = events_subset.copy()\n",
    "    responses_work = responses_subset.copy()\n",
    "    if exclude_special:\n",
    "        events_work = _filter_standard_river_events(events_work)\n",
    "    if events_work.empty:\n",
    "        return pd.DataFrame()\n",
    "    if exclude_special:\n",
    "        responses_work = _filter_standard_river_responses(responses_work)\n",
    "    events_work = events_work.rename(columns={'player': 'bettor'})\n",
    "    base_cols = [\n",
    "        'hand_number',\n",
    "        'bettor',\n",
    "        'line_type',\n",
    "        'ratio',\n",
    "        'bucket',\n",
    "        'bet_amount',\n",
    "        'bet_amount_bb',\n",
    "        'big_blind',\n",
    "        'pot_before',\n",
    "        'bettor_in_position',\n",
    "    ]\n",
    "    missing = [col for col in base_cols if col not in events_work.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns in events dataframe: {', '.join(missing)}\")\n",
    "    events_work = events_work[base_cols].copy()\n",
    "    if responses_work.empty:\n",
    "        grouped = pd.DataFrame(\n",
    "            columns=[\n",
    "                'hand_number',\n",
    "                'bettor',\n",
    "                'continue_amount',\n",
    "                'call_amount',\n",
    "                'raise_amount',\n",
    "                'call_flag',\n",
    "                'raise_flag',\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        allowed = events_work[['hand_number', 'bettor']].drop_duplicates()\n",
    "        responses_work = responses_work.merge(allowed, on=['hand_number', 'bettor'], how='inner')\n",
    "        if responses_work.empty:\n",
    "            grouped = pd.DataFrame(\n",
    "                columns=[\n",
    "                    'hand_number',\n",
    "                    'bettor',\n",
    "                    'continue_amount',\n",
    "                    'call_amount',\n",
    "                    'raise_amount',\n",
    "                    'call_flag',\n",
    "                    'raise_flag',\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            prepped = responses_work[['hand_number', 'bettor', 'response', 'response_amount']].copy()\n",
    "            prepped['response_amount'] = prepped['response_amount'].fillna(0.0)\n",
    "            prepped['continue_amount'] = np.where(\n",
    "                prepped['response'].isin(['Call', 'Raise']),\n",
    "                prepped['response_amount'],\n",
    "                0.0,\n",
    "            )\n",
    "            prepped['call_amount'] = np.where(\n",
    "                prepped['response'] == 'Call',\n",
    "                prepped['response_amount'],\n",
    "                0.0,\n",
    "            )\n",
    "            prepped['raise_amount'] = np.where(\n",
    "                prepped['response'] == 'Raise',\n",
    "                prepped['response_amount'],\n",
    "                0.0,\n",
    "            )\n",
    "            prepped['call_flag'] = (prepped['response'] == 'Call').astype(int)\n",
    "            prepped['raise_flag'] = (prepped['response'] == 'Raise').astype(int)\n",
    "            grouped = (\n",
    "                prepped.groupby(['hand_number', 'bettor'], as_index=False)\n",
    "                .agg({\n",
    "                    'continue_amount': 'sum',\n",
    "                    'call_amount': 'sum',\n",
    "                    'raise_amount': 'sum',\n",
    "                    'call_flag': 'max',\n",
    "                    'raise_flag': 'max',\n",
    "                })\n",
    "            )\n",
    "    metrics = events_work.merge(grouped, on=['hand_number', 'bettor'], how='left')\n",
    "    for column in ['continue_amount', 'call_amount', 'raise_amount']:\n",
    "        metrics[column] = metrics[column].fillna(0.0)\n",
    "    for column in ['call_flag', 'raise_flag']:\n",
    "        metrics[column] = metrics[column].fillna(0).astype(int)\n",
    "    metrics['continue_flag'] = metrics['continue_amount'] > 0\n",
    "    metrics['call_only_flag'] = (metrics['call_flag'] > 0) & (metrics['raise_flag'] == 0)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        metrics['bet_ratio_pct'] = metrics['ratio'].astype(float) * 100.0\n",
    "        metrics['raise_ratio_pct'] = np.where(\n",
    "            (metrics['raise_flag'] > 0) & (metrics['pot_before'] > 0),\n",
    "            (metrics['raise_amount'] / metrics['pot_before']) * 100.0,\n",
    "            np.nan,\n",
    "        )\n",
    "        metrics['breakeven_fold_pct'] = np.where(\n",
    "            (metrics['bet_amount'] + metrics['pot_before']) > 0,\n",
    "            metrics['bet_amount'] / (metrics['bet_amount'] + metrics['pot_before']) * 100.0,\n",
    "            np.nan,\n",
    "        )\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _compute_river_bucket_metrics(metrics_df, buckets=RIVER_BUCKETS):\n",
    "    results = []\n",
    "    if metrics_df is None or metrics_df.empty:\n",
    "        for _, _, label in buckets:\n",
    "            results.append({'Bucket': label, 'Events': 0.0})\n",
    "        return results\n",
    "    ratios = metrics_df['ratio'].astype(float)\n",
    "    for (low, high, label) in buckets:\n",
    "        if high == float('inf'):\n",
    "            bucket_mask = ratios >= low\n",
    "        else:\n",
    "            bucket_mask = (ratios >= low) & (ratios < high)\n",
    "        bucket_events = metrics_df.loc[bucket_mask]\n",
    "        total = len(bucket_events)\n",
    "        if total == 0:\n",
    "            results.append({'Bucket': label, 'Events': 0.0})\n",
    "            continue\n",
    "        continue_events = int(bucket_events['continue_flag'].sum())\n",
    "        raise_events = int(bucket_events['raise_flag'].sum())\n",
    "        call_only_events = int(bucket_events['call_only_flag'].sum())\n",
    "        continue_pct = continue_events / total * 100.0\n",
    "        raise_pct = raise_events / total * 100.0\n",
    "        call_only_pct = call_only_events / total * 100.0\n",
    "        fold_pct = 100.0 - continue_pct\n",
    "        avg_ratio_pct = _safe_mean(bucket_events['bet_ratio_pct'])\n",
    "        raise_mask = bucket_events['raise_flag'] > 0\n",
    "        avg_raise_ratio_pct = _safe_mean(bucket_events.loc[raise_mask, 'raise_ratio_pct'])\n",
    "        avg_breakeven = _safe_mean(bucket_events['breakeven_fold_pct'])\n",
    "        if np.isnan(avg_ratio_pct):\n",
    "            avg_ratio_pct = 0.0\n",
    "        if np.isnan(avg_raise_ratio_pct):\n",
    "            avg_raise_ratio_pct = 0.0\n",
    "        expected_call_contrib_pct = (call_only_pct * avg_ratio_pct) / 100.0\n",
    "        expected_raise_contrib_pct = (raise_pct * avg_raise_ratio_pct) / 100.0\n",
    "        total_contrib_pct = expected_call_contrib_pct + expected_raise_contrib_pct\n",
    "        row = {\n",
    "            'Bucket': label,\n",
    "            'Events': float(total),\n",
    "            'Fold%': fold_pct,\n",
    "            'Continue%': continue_pct,\n",
    "            'Call-Only%': call_only_pct,\n",
    "            'Raise%': raise_pct,\n",
    "            'Avg Size (Pot%)': avg_ratio_pct,\n",
    "            'Avg Raise Size (Pot%)': avg_raise_ratio_pct,\n",
    "            'Expected Call Contribution (Pot%)': expected_call_contrib_pct,\n",
    "            'Expected Raise Contribution (Pot%)': expected_raise_contrib_pct,\n",
    "            'Raise Contribution Total (Pot%)': total_contrib_pct,\n",
    "            'Breakeven Fold%': avg_breakeven,\n",
    "        }\n",
    "        row['Fold Surplus'] = row['Fold%'] - row['Breakeven Fold%']\n",
    "        results.append(row)\n",
    "    return results\n",
    "\n",
    "\n",
    "VALUE_COLUMNS_RIVER = [\n",
    "    'Bucket',\n",
    "    'Events',\n",
    "    'Continue%',\n",
    "    'Call-Only%',\n",
    "    'Raise%',\n",
    "    'Avg Size (Pot%)',\n",
    "    'Avg Raise Size (Pot%)',\n",
    "    'Expected Call Contribution (Pot%)',\n",
    "    'Expected Raise Contribution (Pot%)',\n",
    "    'Raise Contribution Total (Pot%)',\n",
    "]\n",
    "\n",
    "\n",
    "PRESSURE_COLUMNS_RIVER = [\n",
    "    'Bucket',\n",
    "    'Events',\n",
    "    'Fold%',\n",
    "    'Avg Size (Pot%)',\n",
    "    'Breakeven Fold%',\n",
    "    'Fold Surplus',\n",
    "]\n",
    "\n",
    "\n",
    "_FOLD_SURPLUS_CMAP_RIVER = LinearSegmentedColormap.from_list(\n",
    "    'fold_surplus_cmp_river', ['#b91c1c', '#ffffff', '#15803d']\n",
    ")\n",
    "\n",
    "\n",
    "def _style_river_value_table(table_df, title):\n",
    "    if table_df.empty:\n",
    "        return None\n",
    "    df = table_df.set_index('Bucket').T\n",
    "    percent_rows = [row for row in df.index if row != 'Events']\n",
    "    styled = df.style\n",
    "    if 'Events' in df.index:\n",
    "        styled = styled.format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "    if percent_rows:\n",
    "        styled = styled.format('{:.1f}%', subset=pd.IndexSlice[percent_rows, :])\n",
    "        contribution_rows = [row for row in percent_rows if 'Contribution' in row]\n",
    "        size_rows = [row for row in percent_rows if 'Size' in row]\n",
    "        pct_rows_other = [row for row in percent_rows if row not in contribution_rows + size_rows]\n",
    "        if pct_rows_other:\n",
    "            styled = styled.background_gradient(\n",
    "                cmap='YlOrRd',\n",
    "                axis=None,\n",
    "                subset=pd.IndexSlice[pct_rows_other, :],\n",
    "            )\n",
    "        if size_rows:\n",
    "            styled = styled.background_gradient(\n",
    "                cmap='BuGn',\n",
    "                axis=None,\n",
    "                subset=pd.IndexSlice[size_rows, :],\n",
    "            )\n",
    "        if contribution_rows:\n",
    "            styled = styled.background_gradient(\n",
    "                cmap='PuBu',\n",
    "                axis=None,\n",
    "                subset=pd.IndexSlice[contribution_rows, :],\n",
    "            )\n",
    "    return styled.set_caption(title)\n",
    "\n",
    "\n",
    "def _style_river_pressure_table(table_df, title):\n",
    "    if table_df.empty:\n",
    "        return None\n",
    "    df = table_df.set_index('Bucket').T\n",
    "    desired_order = ['Events', 'Fold%', 'Avg Size (Pot%)', 'Breakeven Fold%', 'Fold Surplus']\n",
    "    existing_order = [row for row in desired_order if row in df.index]\n",
    "    df = df.loc[existing_order]\n",
    "    styled = df.style\n",
    "    if 'Events' in df.index:\n",
    "        styled = styled.format('{:.0f}', subset=pd.IndexSlice[['Events'], :])\n",
    "    percent_rows = [row for row in df.index if row != 'Events']\n",
    "    if percent_rows:\n",
    "        styled = styled.format('{:.1f}%', subset=pd.IndexSlice[percent_rows, :])\n",
    "    if 'Fold%' in df.index:\n",
    "        styled = styled.background_gradient(\n",
    "            cmap='YlOrRd',\n",
    "            axis=None,\n",
    "            subset=pd.IndexSlice[['Fold%'], :],\n",
    "        )\n",
    "    if 'Avg Size (Pot%)' in df.index:\n",
    "        styled = styled.background_gradient(\n",
    "            cmap='BuGn',\n",
    "            axis=None,\n",
    "            subset=pd.IndexSlice[['Avg Size (Pot%)'], :],\n",
    "        )\n",
    "    if 'Fold Surplus' in df.index:\n",
    "        fold_values = df.loc['Fold Surplus'].dropna().astype(float)\n",
    "        if not fold_values.empty:\n",
    "            max_abs = max(abs(fold_values.min()), abs(fold_values.max()))\n",
    "            if max_abs == 0:\n",
    "                max_abs = 1.0\n",
    "            styled = styled.background_gradient(\n",
    "                cmap=_FOLD_SURPLUS_CMAP_RIVER,\n",
    "                axis=None,\n",
    "                subset=pd.IndexSlice[['Fold Surplus'], :],\n",
    "                vmin=-max_abs,\n",
    "                vmax=max_abs,\n",
    "            )\n",
    "    return styled.set_caption(title)\n",
    "\n",
    "\n",
    "def _display_river_event_tables(metrics_df, title_prefix):\n",
    "    if metrics_df is None or metrics_df.empty:\n",
    "        print(f'No events available for {title_prefix}.')\n",
    "        return\n",
    "    records = _compute_river_bucket_metrics(metrics_df)\n",
    "    if not records:\n",
    "        print(f'No bucketed metrics available for {title_prefix}.')\n",
    "        return\n",
    "    metrics_table = pd.DataFrame(records)\n",
    "    value_columns = [col for col in VALUE_COLUMNS_RIVER if col in metrics_table.columns]\n",
    "    pressure_columns = [col for col in PRESSURE_COLUMNS_RIVER if col in metrics_table.columns]\n",
    "    value_table = metrics_table[value_columns]\n",
    "    pressure_table = metrics_table[pressure_columns]\n",
    "    value_styled = _style_river_value_table(value_table, f\"{title_prefix} - Value Profile\")\n",
    "    if value_styled is not None:\n",
    "        display(value_styled)\n",
    "    pressure_styled = _style_river_pressure_table(pressure_table, f\"{title_prefix} - Fold Pressure\")\n",
    "    if pressure_styled is not None:\n",
    "        display(pressure_styled)\n",
    "\n",
    "\n",
    "def display_river_event_ev_tables(line_type, title_prefix, *, exclude_special=True):\n",
    "    line_events = events_df[events_df['line_type'] == line_type]\n",
    "    if line_events.empty:\n",
    "        print(f'No {line_type.lower()} events available for analysis.')\n",
    "        return\n",
    "    line_responses = responses_df[responses_df['line_type'] == line_type]\n",
    "    base_metrics = _prepare_river_event_metrics(line_events, line_responses, exclude_special=exclude_special)\n",
    "    if base_metrics.empty:\n",
    "        print(f'No {line_type.lower()} events remain after filtering special cases.')\n",
    "        return\n",
    "    subsets = [\n",
    "        (base_metrics, f\"{title_prefix} (Overall)\"),\n",
    "        (base_metrics[base_metrics['bettor_in_position']], f\"{title_prefix} (In Position)\"),\n",
    "        (base_metrics[~base_metrics['bettor_in_position']], f\"{title_prefix} (Out of Position)\"),\n",
    "    ]\n",
    "    for subset_df, label in subsets:\n",
    "        if subset_df is None or subset_df.empty:\n",
    "            continue\n",
    "        _display_river_event_tables(subset_df, label)\n",
    "\n",
    "\n",
    "def display_all_river_event_tables():\n",
    "    if events_df.empty or responses_df.empty:\n",
    "        print('No river events or responses available for event-level contribution tables.')\n",
    "        return\n",
    "    line_specs = [\n",
    "        ('Triple Barrel', 'River Triple Barrel Event Outcomes'),\n",
    "        ('Flop-River', 'River Flop-River Event Outcomes'),\n",
    "        ('Turn Follow-up', 'River Turn Follow-up Event Outcomes'),\n",
    "        ('River Only', 'River Only Event Outcomes'),\n",
    "    ]\n",
    "    for line_type, title in line_specs:\n",
    "        display_river_event_ev_tables(line_type, title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c352270",
   "metadata": {},
   "source": [
    "### River Event-Level Outcomes\n",
    "\n",
    "River bet sizes summarised as pot-percent contributions with special cases removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "if events_df.empty or responses_df.empty:\n",
    "    print('No river events or responses available for event-level analysis.')\n",
    "else:\n",
    "    display_all_river_event_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c632e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_line_comparison(events_df, \"River Hand Types by Line\")\n",
    "display_line_comparison_grouped(events_df, \"River Hand Types by Line (Grouped)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bucket_table(events_df, 'River Bet Sizing (All Lines)')\n",
    "display_bucket_table(events_df[events_df[\"line_type\"] == \"Triple Barrel\"], \"River Bet Sizing (Triple Barrel)\")\n",
    "display_bucket_table(events_df[events_df[\"line_type\"] == \"Flop-River\"], \"River Bet Sizing (Flop-River)\")\n",
    "display_bucket_table(events_df[events_df[\"line_type\"] == \"Turn Follow-up\"], \"River Bet Sizing (Turn Follow-up)\")\n",
    "display_bucket_table(events_df[events_df[\"line_type\"] == \"River Only\"], \"River Bet Sizing (River Only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2484f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response_table(\"Call\")\n",
    "display_response_table(\"Raise\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
